# 2. [系统需求](http://dpdk.org/doc/guides/linux_gsg/sys_reqs.html)

本章介绍了编译DPDK需要的软件包。

> **注意**
>
> 如果DPDK在英特尔®通信芯片组89xx系列平台上使用，请参阅*英特尔®通信芯片组89xx系列软件Linux入门指南*。

## 2.1. x86平台BOIOS设定前提

对于大多数平台，使用DPDK的基础功能不需要特殊的BIOS设置。但是，对于额外的HPET定时器和电源管理功能以及40G网卡上的小数据包的高性能，可能需要进行更改BIOS设置。有关所需更改的更多信息，请参阅[启动附加功能](https://github.com/answerwa/DPDK-translation/blob/master/针对Linux的入门指南/5.%20启用附加功能.md)部分。

## 2.2. DPDK的编译

**所需工具：**

> **注意**
>
> 已使用Fedora 18进行过测试。其它系统上所需安装命令和安装包可能有出入。有关其它Linux发行版和已测试版本的详细信息，请参阅DPDK发行说明。

- GNU `make` 。

- coreutils: `cmp`, `sed`, `grep`, `arch` 等。

- gcc: 对于所有平台，建议使用4.9或更高版本。在某些发行版中，会默认启用一些特定的编译器标志和连接器标志，会影响性能（例如 `-fstack-protector` ）。请参阅使用的发行版文档和 `gcc -dumpspecs` 。

- libc头文件，通常打包为`gcc-multilib` （`glibc-devel.i686` / `libc6-dev-i386`；`glibc-devel.x86_64` / `libc6-dev` 适用Intel架构的64位编译； `glibc-devel.ppc64` 使用IBM Power架构的64位编译）。

- 构建内核模块需要的Linux内核头文件和源码。 (kernel - devel.x86_64; kernel - devel.ppc64)

- 在64位系统上进行32位编译所需的其它软件包是：

  - glibc.i686, libgcc.i686, libstdc++.i686 and glibc-devel.i686 for Intel i686/x86_64;
  - glibc.ppc64, libgcc.ppc64, libstdc++.ppc64 and glibc-devel.ppc64 for IBM ppc_64;

  > **注意**
  >
  > x86_x32 ABI目前仅在高于Ubuntu 13.10的发行版和Debian最近的发行版上支持。只支持的编译器是gcc 4.9+。

- Python, version 2.7+ or 3.2+，使用DPDK包中的各种帮助脚本。

**可选工具：**

- Intel® C++ Compiler (icc). 可能需要额外的库用于安装。参考编译器安装文件目录下的icc安装指南。
- IBM® Advance ToolChain for Powerlinux. 这是一组开源开发工具和运行时库，可以让用户在Linux上获得IBM最新的POWER硬件功能的优势。要安装它，请参阅IBM官方安装文档。
- libpcap头文件和库(libpcap-devel)，用来编译和使用libpcap-based poll-mode驱动。此驱动默认禁用，可以在编译时，通过在配置文件中设置`CONFIG_RTE_LIBRTE_PMD_PCAP=y`来启用。
- libarchive头文件和库，在某些单元测试中会被用到，可以通过tar来获取这些资源。

## 2.3. 运行DPDK程序

要运行DPDK程序，可能需要在目标机器上进行某些定制。

### 2.3.1. 系统软件

**需求**

- Kernel version >= 2.6.34

  使用的内核版本可以用此命令查看：

  ```
  uname -r

  ```

- glibc >= 2.7 (cpuset相关功能)

  可以使用 `ldd --version` 检查此版本。

- 内核配置

  在Fedora OS和其它常用发行版中，例如Ubuntu，Red Hat Enterprise Linux，供应商提供的内核配置可用于运行大多数DPDK程序。

  对于其它的内核构建，应为DPDK启用的选项包括：

  - 支持UIO
  - 支持HUGETLBFS
  - 支持PROC_PAGE_MONITOR
  - 如果需要支持HPET，还应启用HPET和HPET_MMAP配置选项。有关更多详细信息，请参阅[高精度事件计时器（HPET）功能](https://github.com/answerwa/DPDK-translation/blob/master/针对Linux的入门指南/5.%20启用附加功能.md#51-高精度事件计时器（HPET）功能)部分。

### 2.3.2. 在Linux环境中使用Hugepages。

Hugepage support is required for the large memory pool allocation used for packet buffers (the HUGETLBFS option must be enabled in the running kernel as indicated the previous section). By using hugepage allocations, performance is increased since fewer pages are needed, and therefore less Translation Lookaside Buffers (TLBs, high speed translation caches), which reduce the time it takes to translate a virtual page address to a physical page address. Without hugepages, high TLB miss rates would occur with the standard 4k page size, slowing performance.

#### 2.3.2.1. Reserving Hugepages for DPDK Use

The allocation of hugepages should be done at boot time or as soon as possible after system boot to prevent memory from being fragmented in physical memory. To reserve hugepages at boot time, a parameter is passed to the Linux kernel on the kernel command line.

For 2 MB pages, just pass the hugepages option to the kernel. For example, to reserve 1024 pages of 2 MB, use:

```
hugepages=1024

```

For other hugepage sizes, for example 1G pages, the size must be specified explicitly and can also be optionally set as the default hugepage size for the system. For example, to reserve 4G of hugepage memory in the form of four 1G pages, the following options should be passed to the kernel:

```
default_hugepagesz=1G hugepagesz=1G hugepages=4

```

Note

The hugepage sizes that a CPU supports can be determined from the CPU flags on Intel architecture. If pse exists, 2M hugepages are supported; if pdpe1gb exists, 1G hugepages are supported. On IBM Power architecture, the supported hugepage sizes are 16MB and 16GB.

Note

For 64-bit applications, it is recommended to use 1 GB hugepages if the platform supports them.

In the case of a dual-socket NUMA system, the number of hugepages reserved at boot time is generally divided equally between the two sockets (on the assumption that sufficient memory is present on both sockets).

See the Documentation/kernel-parameters.txt file in your Linux source tree for further details of these and other kernel options.

**Alternative:**

For 2 MB pages, there is also the option of allocating hugepages after the system has booted. This is done by echoing the number of hugepages required to a nr_hugepages file in the `/sys/devices/`directory. For a single-node system, the command to use is as follows (assuming that 1024 pages are required):

```
echo 1024 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages

```

On a NUMA machine, pages should be allocated explicitly on separate nodes:

```
echo 1024 > /sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages
echo 1024 > /sys/devices/system/node/node1/hugepages/hugepages-2048kB/nr_hugepages

```

Note

For 1G pages, it is not possible to reserve the hugepage memory after the system has booted.

On IBM POWER system, the nr_overcommit_hugepages should be set to the same value as nr_hugepages. For example, if the required page number is 128, the following commands are used:

```
echo 128 > /sys/kernel/mm/hugepages/hugepages-16384kB/nr_hugepages
echo 128 > /sys/kernel/mm/hugepages/hugepages-16384kB/nr_overcommit_hugepages

```

#### 2.3.2.2. Using Hugepages with the DPDK

Once the hugepage memory is reserved, to make the memory available for DPDK use, perform the following steps:

```
mkdir /mnt/huge
mount -t hugetlbfs nodev /mnt/huge

```

The mount point can be made permanent across reboots, by adding the following line to the `/etc/fstab` file:

```
nodev /mnt/huge hugetlbfs defaults 0 0

```

For 1GB pages, the page size must be specified as a mount option:

```
nodev /mnt/huge_1GB hugetlbfs pagesize=1GB 0 0

```

### 2.3.3. Xen Domain0 Support in the Linux Environment

The existing memory management implementation is based on the Linux kernel hugepage mechanism. On the Xen hypervisor, hugepage support for DomainU (DomU) Guests means that DPDK applications work as normal for guests.

However, Domain0 (Dom0) does not support hugepages. To work around this limitation, a new kernel module rte_dom0_mm is added to facilitate the allocation and mapping of memory via **IOCTL** (allocation) and **MMAP** (mapping).

#### 2.3.3.1. Enabling Xen Dom0 Mode in the DPDK

By default, Xen Dom0 mode is disabled in the DPDK build configuration files. To support Xen Dom0, the CONFIG_RTE_LIBRTE_XEN_DOM0 setting should be changed to “y”, which enables the Xen Dom0 mode at compile time.

Furthermore, the CONFIG_RTE_EAL_ALLOW_INV_SOCKET_ID setting should also be changed to “y” in the case of the wrong socket ID being received.

#### 2.3.3.2. Loading the DPDK rte_dom0_mm Module

To run any DPDK application on Xen Dom0, the `rte_dom0_mm` module must be loaded into the running kernel with rsv_memsize option. The module is found in the kmod sub-directory of the DPDK target directory. This module should be loaded using the insmod command as shown below (assuming that the current directory is the DPDK target directory):

```
sudo insmod kmod/rte_dom0_mm.ko rsv_memsize=X

```

The value X cannot be greater than 4096(MB).

#### 2.3.3.3. Configuring Memory for DPDK Use

After the rte_dom0_mm.ko kernel module has been loaded, the user must configure the memory size for DPDK usage. This is done by echoing the memory size to a memsize file in the /sys/devices/ directory. Use the following command (assuming that 2048 MB is required):

```
echo 2048 > /sys/kernel/mm/dom0-mm/memsize-mB/memsize

```

The user can also check how much memory has already been used:

```
cat /sys/kernel/mm/dom0-mm/memsize-mB/memsize_rsvd

```

Xen Domain0 does not support NUMA configuration, as a result the `--socket-mem` command line option is invalid for Xen Domain0.

Note

The memsize value cannot be greater than the rsv_memsize value.

#### 2.3.3.4. Running the DPDK Application on Xen Domain0

To run the DPDK application on Xen Domain0, an extra command line option `--xen-dom0` is required.