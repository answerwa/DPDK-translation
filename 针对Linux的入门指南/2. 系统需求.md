# 2. [系统需求](http://dpdk.org/doc/guides/linux_gsg/sys_reqs.html)

本章介绍了编译DPDK需要的软件包。

> **注意**
>
> 如果DPDK在英特尔®通信芯片组89xx系列平台上使用，请参阅*英特尔®通信芯片组89xx系列软件Linux入门指南*。

## 2.1. x86平台BOIOS设定前提

对于大多数平台，使用DPDK的基础功能不需要特殊的BIOS设置。但是，对于额外的HPET定时器和电源管理功能以及40G网卡上的小数据包的高性能，可能需要进行更改BIOS设置。有关所需更改的更多信息，请参阅[启动附加功能](https://github.com/answerwa/DPDK-translation/blob/master/%E9%92%88%E5%AF%B9Linux%E7%9A%84%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/5.%20%E5%90%AF%E7%94%A8%E9%99%84%E5%8A%A0%E5%8A%9F%E8%83%BD.md#5-启用附加功能)部分。

## 2.2. DPDK的编译

**所需工具：**

> **注意**
>
> 已使用Fedora 18进行过测试。其它系统上所需安装命令和安装包可能有出入。有关其它Linux发行版和已测试版本的详细信息，请参阅DPDK发行说明。

- GNU `make` 。

- coreutils: `cmp`, `sed`, `grep`, `arch` 等。

- gcc: 对于所有平台，建议使用4.9或更高版本。在某些发行版中，会默认启用一些特定的编译器标志和连接器标志，会影响性能（例如 `-fstack-protector` ）。请参阅使用的发行版文档和 `gcc -dumpspecs` 。

- libc头文件，通常打包为`gcc-multilib` （`glibc-devel.i686` / `libc6-dev-i386`；`glibc-devel.x86_64` / `libc6-dev` 适用Intel架构的64位编译； `glibc-devel.ppc64` 使用IBM Power架构的64位编译）。

- 构建内核模块需要的Linux内核头文件和源码。 (kernel - devel.x86_64; kernel - devel.ppc64)

- 在64位系统上进行32位编译所需的其它软件包是：

  - glibc.i686, libgcc.i686, libstdc++.i686 and glibc-devel.i686 for Intel i686/x86_64;
  - glibc.ppc64, libgcc.ppc64, libstdc++.ppc64 and glibc-devel.ppc64 for IBM ppc_64;

  > **注意**
  >
  > x86_x32 ABI目前仅在高于Ubuntu 13.10的发行版和Debian最近的发行版上支持。只支持的编译器是gcc 4.9+。

- Python, version 2.7+ or 3.2+，使用DPDK包中的各种帮助脚本。

**可选工具：**

- Intel® C++ Compiler (icc). 可能需要额外的库用于安装。参考编译器安装文件目录下的icc安装指南。
- IBM® Advance ToolChain for Powerlinux. 这是一组开源开发工具和运行时库，可以让用户在Linux上获得IBM最新的POWER硬件功能的优势。要安装它，请参阅IBM官方安装文档。
- libpcap头文件和库(libpcap-devel)，用来编译和使用libpcap-based poll-mode驱动。此驱动默认禁用，可以在编译时，通过在配置文件中设置`CONFIG_RTE_LIBRTE_PMD_PCAP=y`来启用。
- libarchive头文件和库，在某些单元测试中会被用到，可以通过tar来获取这些资源。

## 2.3. 运行DPDK程序

要运行DPDK程序，可能需要在目标机器上进行某些定制。

### 2.3.1. 系统软件

**需求**

- Kernel version >= 2.6.34

  使用的内核版本可以用此命令查看：

  ```sh
  uname -r
  ```

- glibc >= 2.7 (cpuset相关功能)

  可以使用 `ldd --version` 检查此版本。

- 内核配置

  在Fedora OS和其它常用发行版中，例如Ubuntu，Red Hat Enterprise Linux，供应商提供的内核配置可用于运行大多数DPDK程序。

  对于其它的内核构建，应为DPDK启用的选项包括：

  - 支持UIO
  - 支持HUGETLBFS
  - 支持PROC_PAGE_MONITOR
  - 如果需要支持HPET，还应启用HPET和HPET_MMAP配置选项。有关更多详细信息，请参阅[高精度事件计时器（HPET）功能](https://github.com/answerwa/DPDK-translation/blob/master/%E9%92%88%E5%AF%B9Linux%E7%9A%84%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/5.%20%E5%90%AF%E7%94%A8%E9%99%84%E5%8A%A0%E5%8A%9F%E8%83%BD.md#51-高精度事件定时器hpet功能)部分。

### 2.3.2. 在Linux环境中使用Hugepages

用于数据包缓冲区的大内存池分配需要支持Hugepage（必须在运行的内核中启用HUGETLBFS选项，如上一节所述）。通过使用hugepage分配，性能增加，因为需要更少的页面，因此需要更少的Translation Lookaside Buffers（TLBs，高速翻译高速缓存），减少了将虚拟页面地址翻译成物理页面地址所需的时间。没有hugepages，标准4k页面大小会导致高TLB错失率，性能下降。

#### 2.3.2.1. 为DPDK的使用保留Hugepages

hugepages的分配应在启动时或系统启动后尽快进行，以防止内存在物理内存中分段。要在启动时保留hugepages，需要传递一个参数给内核命令行上的Linux内核。

对于2 MB页面，只需将hugepages选项传递给内核。 例如，要保留1024个2 MB的页面，请使用：

```sh
hugepages=1024
```

对于其它的hugepage大小，例如1G页面，必须明确指定大小，也可以选择将其设置为系统的默认hugepage大小。例如，要以四个1G页面的形式预留4G的hugepage内存，应将以下选项传递给内核：

```sh
default_hugepagesz=1G hugepagesz=1G hugepages=4
```

> **注意**
>
> CPU支持的主机大小可以从Intel架构的CPU标志中确定。 如果存在pse，则支持2M的hugepages; 如果存在pdpe1gb，则支持1G的hugepages。 在IBM Power架构中，支持的hugepage大小为16MB和16GB。

>**注意**
>
>对于64位应用程序，如果平台支持，建议使用1 GB的hugepages。

在双插槽NUMA系统的情况下，启动时hugepages的保留数量通常在两个插槽之间平均分配（假设两个插槽上都有足够的内存）。

有关这些和其他内核选项的更多详细信息，请参阅Linux源代码树中的Documentation / kernel-parameters.txt文件。

**替代方案：**

对于2 MB页面，还可以在系统启动后分配hugepages。 这是通过回调`/sys/devices/`目录中nr_hugepages文件所需的hugepages数来完成的。 对于单节点系统，使用的命令如下（假设需要1024个页面）：

```sh
echo 1024 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
```

在NUMA机器上，应该在单独的节点上显式分配页面：

```sh
echo 1024 > /sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages
echo 1024 > /sys/devices/system/node/node1/hugepages/hugepages-2048kB/nr_hugepages
```

> **注意**
>
> 对于1G页面，系统启动后无法保留页面内存。
>
> 在IBM POWER系统上，nr_overcommit_hugepages应该设置为与nr_hugepages相同的值。 例如，如果所需的页码为128，则使用以下命令：
>
>
> ```sh
> echo 128 > /sys/kernel/mm/hugepages/hugepages-16384kB/nr_hugepages
> echo 128 > /sys/kernel/mm/hugepages/hugepages-16384kB/nr_overcommit_hugepages
> ```
>

#### 2.3.2.2. 在DPDK中使用Hugepages 

一旦保留了hugepage内存，为了使内存可用于DPDK，请执行以下步骤：

```sh
mkdir /mnt/huge
mount -t hugetlbfs nodev /mnt/huge
```

通过将以下行添加到`/etc/fstab` 文件中，挂载点可以在重新启动之后永久保存：

```sh
nodev /mnt/huge hugetlbfs defaults 0 0
```

对于1GB页面，页面大小必须指定为挂载选项选项：

```sh
nodev /mnt/huge_1GB hugetlbfs pagesize=1GB 0 0
```

### 2.3.3. Xen Domain0在Linux环境中的支持

现有的内存管理实现是基于Linux内核的hugepage机制的。在Xen虚拟机管理程序中，hugepage支持DomainU（DomU）客户端意味着DPDK应用程序与客户端一样正常工作。

但是，Domain0（Dom0）不支持hugepages。 为了解决这个限制，添加了一个新的内核模块rte_dom0_mm以便于通过**IOCTL**（分配）和**MMAP**（映射）对内存进行分配和映射。

#### 2.3.3.1. 在DPDK中启用Xen Dom0模式

默认情况下，DPDK构建配置文件中禁用Xen Dom0模式。 要支持Xen Dom0，CONFIG_RTE_LIBRTE_XEN_DOM0设置应该更改为“y”，这将在编译时启用Xen Dom0模式。

此外，以防收到错误的套接字ID，CONFIG_RTE_EAL_ALLOW_INV_SOCKET_ID设置也应更改为“y”。

要在Xen Dom0上运行任意DPDK应用程序，`rte_dom0_mm`模块必须使用rsv_memsize选项加载到正在运行的内核中。该模块位于DPDK目标目录的kmod子目录中。 应使用insmod命令加载此模块，如下所示（假设当前目录是DPDK目标目录）：

```sh
sudo insmod kmod/rte_dom0_mm.ko rsv_memsize=X
```

X的值不能大于4096（MB）。

#### 2.3.3.3. 为DPDK的使用配置内存

在加载rte_dom0_mm.ko内核模块之后，用户必须配置DPDK使用的内存大小。通过将内存大小回写到`/sys/devices/`目录中的memsize文件来完成。 使用以下命令（假设需要2048 MB）：

```sh
echo 2048 > /sys/kernel/mm/dom0-mm/memsize-mB/memsize
```

用户还可以检查已经使用了多少内存：

```sh
cat /sys/kernel/mm/dom0-mm/memsize-mB/memsize_rsvd
```

Xen Domain0不支持NUMA配置，因此--socket-mem命令行选项对Xen Domain0无效。

>  **注意**
>
>  memsize值不能大于rsv_memsize值。

#### 2.3.3.4. 在Xen Domain0上运行DPDK应用程序

为了在Xen Domain0上运行DPDK应用程序，需要额外设置`--xen-dom0` 这个命令行选项。