# 7. [如何通过Intel平台上的NIC获得最佳性能](http://dpdk.org/doc/guides/linux_gsg/nic_perf_intel_platform.html)

本文档是从Intel平台上的DPDK应用程序获得高性能的分步指南。

## 7.1. 硬件和内存要求

为了获得最佳性能，请使用Intel Xeon级服务器系统，如Ivy Bridge，Haswell或更高版本。

确保每个内存通道至少插入一个内存DIMM，每个内存通道的内存大小至少为4GB。 **注意：**这对性能有最直接的影响。

您可以使用`dmidecode`检查内存配置，如下所示：

```sh
dmidecode -t memory | grep Locator

Locator: DIMM_A1
Bank Locator: NODE 1
Locator: DIMM_A2
Bank Locator: NODE 1
Locator: DIMM_B1
Bank Locator: NODE 1
Locator: DIMM_B2
Bank Locator: NODE 1
...
Locator: DIMM_G1
Bank Locator: NODE 2
Locator: DIMM_G2
Bank Locator: NODE 2
Locator: DIMM_H1
Bank Locator: NODE 2
Locator: DIMM_H2
Bank Locator: NODE 2
```

上面的示例输出显示了8个通道，从`A`到`H`，每个通道都有2个DIMM。

您还可以使用`dmidecode`来确定内存频率：

```sh
dmidecode -t memory | grep Speed

Speed: 2133 MHz
Configured Clock Speed: 2134 MHz
Speed: Unknown
Configured Clock Speed: Unknown
Speed: 2133 MHz
Configured Clock Speed: 2134 MHz
Speed: Unknown
...
Speed: 2133 MHz
Configured Clock Speed: 2134 MHz
Speed: Unknown
Configured Clock Speed: Unknown
Speed: 2133 MHz
Configured Clock Speed: 2134 MHz
Speed: Unknown
Configured Clock Speed: Unknown
```

输出显示2133 MHz（DDR4）和未知（不存在）的速度。 这与先前的输出一致，表明每个通道都有一个记忆棒。

### 7.1.1. 网卡要求

使用[DPDK支持](http://dpdk.org/doc/nics)的高端NIC，如Intel XL710 40GbE。

确保每个NIC已经刷新最新版本的NVM /固件。

使用PCIe Gen3插槽，如Gen3 `x8`或Gen3 `x16`，因为PCIe Gen2插槽不能为2 x 10GbE及以上的带宽提供足够的带宽。 您可以使用`lspci`来检查PCI插槽的速度，方法如下：

```sh
lspci -s 03:00.1 -vv | grep LnkSta

LnkSta: Speed 8GT/s, Width x8, TrErr- Train- SlotClk+ DLActive- ...
LnkSta2: Current De-emphasis Level: -6dB, EqualizationComplete+ ...
```

将NIC插入PCI插槽时，始终检查标签，如CPU0或CPU1，以指示其连接的插槽。

插槽为NUMA情况时应该注意。 如果从不同的NIC使用2个或更多端口，最好确保这些NIC位于同一个CPU插槽上。 下面进一步显示了如何确定这一点的示例。

### 7.1.2. BIOS设置

以下是有关BIOS设置的一些建议。 不同的平台具有不同的BIOS命名，所以以下主要用于参考：

1. 开始之前，请考虑将所有BIOS设置重置为默认值。
2. 禁用所有省电选项，如电源性能调整， CPU P-State， CPU C3 Report 和 CPU C6 Report。
3. 选择**Performance**作为CPU Power和Performance 策略。
4. 禁用Turbo Boost以确保性能缩放随核心数量的增加而增加。
5. 将内存频率设置为最高可用值，不自动。
6. 测试NIC的物理功能时禁用所有虚拟化选项，如果要使用VFIO，请打开`VT-d`。

### 7.1.3. Linux启动命令行

以下是有关GRUB启动设置的一些建议：

1. 使用默认的grub文件作为起点。

2. 通过grub配置保留1G hugepages。 例如，保留8个1G的hugepages：

   ```sh
   default_hugepagesz=1G hugepagesz=1G hugepages=8
   ```

3. 隔离将用于DPDK的CPU内核。 例如：

   ```sh
   isolcpus=2,3,4,5,6,7,8
   ```

4. 如果要使用VFIO，请使用以下附加的grub参数：

   ```sh
   iommu=pt intel_iommu=on
   ```

## 7.2. 运行DPDK之前的配置

1. 构建DPDK目标并保留hugepages。有关更多详细信息，请参阅前面有关[在Linux环境中使用Hugepages](https://github.com/answerwa/DPDK-translation/blob/master/%E9%92%88%E5%AF%B9Linux%E7%9A%84%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/2.%20%E7%B3%BB%E7%BB%9F%E9%9C%80%E6%B1%82.md#232-在linux环境中使用hugepages)部分。

   以下shell命令可能有助于构建和配置：

   ```sh
   # Build DPDK target.
   cd dpdk_folder
   make install T=x86_64-native-linuxapp-gcc -j

   # Get the hugepage size.
   awk '/Hugepagesize/ {print $2}' /proc/meminfo

   # Get the total huge page numbers.
   awk '/HugePages_Total/ {print $2} ' /proc/meminfo

   # Unmount the hugepages.
   umount `awk '/hugetlbfs/ {print $2}' /proc/mounts`

   # Create the hugepage mount folder.
   mkdir -p /mnt/huge

   # Mount to the specific folder.
   mount -t hugetlbfs nodev /mnt/huge
   ```

2. 使用DPDK `cpu_layout`实用程序检查CPU布局：

   ```sh
   cd dpdk_folder

   usertools/cpu_layout.py
   ```

   或者运行`lscpu`来检查每个插槽的核心。

3. 检查您的NIC ID和相关的插槽ID：

   ```sh
   # List all the NICs with PCI address and device IDs.
   lspci -nn | grep Eth
   ```

   例如，假设你的输出如下：

   ```sh
   82:00.0 Ethernet [0200]: Intel XL710 for 40GbE QSFP+ [8086:1583]
   82:00.1 Ethernet [0200]: Intel XL710 for 40GbE QSFP+ [8086:1583]
   85:00.0 Ethernet [0200]: Intel XL710 for 40GbE QSFP+ [8086:1583]
   85:00.1 Ethernet [0200]: Intel XL710 for 40GbE QSFP+ [8086:1583]
   ```

   检查PCI设备相关的numa节点ID：

   ```sh
   cat /sys/bus/pci/devices/0000\:xx\:00.x/numa_node
   ```

   通常`0x:00.x`在插槽0上和`8x:00.x`在插槽1上。**注意：**要获得最佳性能，请确保核心和NIC位于同一个插槽中。在上面的例子中，`85:00.0`在插槽1上，应由插槽1上的核心使用，以获得最佳性能。

4. 将测试端口绑定到DPDK兼容的驱动程序，如igb_uio。 例如，将两个端口绑定到兼容DPDK的驱动程序并检查状态：

   ```sh
   # Bind ports 82:00.0 and 85:00.0 to dpdk driver
   ./dpdk_folder/usertools/dpdk-devbind.py -b igb_uio 82:00.0 85:00.0

   # Check the port driver status
   ./dpdk_folder/usertools/dpdk-devbind.py --status
   ```

   有关详细信息，请参阅`dpdk-devbind.py --help`。

更多有关DPDK设置和Linux内核要求的详细信息，请参阅[从源码编译DPDK目标](https://github.com/answerwa/DPDK-translation/blob/master/%E9%92%88%E5%AF%B9Linux%E7%9A%84%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97/3.%20%E4%BB%8E%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91DPDK%E7%9B%AE%E6%A0%87.md#3-从源码编译dpdk目标)。

## 7.3. 获取Intel NIC最佳性能的示例

以下是运行DPDK `l3fwd`示例应用程序以获得Intel服务器平台和Intel XL710 NICs的高性能的示例。具体的40G网卡配置请参考i40e网卡指南。

示例场景是通过两个Intel XL710 40GbE端口获得最佳性能。见[图7.1](http://dpdk.org/doc/guides/linux_gsg/nic_perf_intel_platform.html#figure-intel-perf-test-setup)进行性能测试设置。

![http://dpdk.org/doc/guides/_images/intel_perf_test_setup.svg]()

*图7.1 性能测试设置*

1. 将两个Intel XL710 NIC添加到该平台，并使用每个卡的一个端口来获得最佳性能。使用两个NIC的原因是克服PCIe Gen3的限制，因为它不能为两个40G端口提供80G带宽，但两个不同的PCIe Gen3 x8插槽可以。参考上面的NIC输出示例，我们可以选择`82:00.0`和`85:00.0` 作为测试端口：

   ```sh
   82:00.0 Ethernet [0200]: Intel XL710 for 40GbE QSFP+ [8086:1583]
   85:00.0 Ethernet [0200]: Intel XL710 for 40GbE QSFP+ [8086:1583]
   ```

2. 将端口连接到流量生成器。对于高速测试，最好使用硬件流量生成器。

3. 检查PCI设备numa节点（插槽ID），并获取准确插槽ID上的核心号。在这种情况下，`82:00.0`和`85:00.0` 都在插槽1中，引用平台的插槽1上的核心是18-35和54-71。注意：不要在同一个核上使用2个逻辑内核（例如，core18有2个逻辑内核，core18和core54），而是使用不同核的2个逻辑内核（例如core18和core19）。

4. 将这两个端口绑定到igb_uio。

5. 对于XL710 40G端口，我们需要至少两个队列对来实现最佳性能，每个端口需要两个队列，每个队列对都需要专用的CPU内核来接收/发送数据包。

6. DPDK示例应用程序`l3fwd`将用于性能测试，使用两个端口进行双向转发。使用默认lpm模式编译`l3fwd sample`。

7. 运行l3fwd的命令类似如下：

   ```sh
   ./l3fwd -l 18-21 -n 4 -w 82:00.0 -w 85:00.0 \
           -- -p 0x3 --config '(0,0,18),(0,1,19),(1,0,20),(1,1,21)'
   ```

   表示应用程序使用核心18为端口0，队列对0进行转发；核心19为端口0，队列1进行转发；核心20为端口1，队列对0进行转发；核心21为端口1，队列1进行转发。

8. 在流量生成器上配置流量。

   - 开始在包生成器上创建流。
   - 将Ethernet II类型设置为0x0800。