# 7. [如何通过Intel平台上的NIC获得最佳性能](http://dpdk.org/doc/guides/linux_gsg/nic_perf_intel_platform.html)

本文档是从Intel平台上的DPDK应用程序获得高性能的分步指南。

## 7.1. 硬件和内存要求

为了获得最佳性能，请使用Intel Xeon级服务器系统，如Ivy Bridge，Haswell或更高版本。

确保每个内存通道至少插入一个内存DIMM，每个内存通道的内存大小至少为4GB。 **注意：**这对性能有最直接的影响。

您可以使用`dmidecode`检查内存配置，如下所示：

```sh
dmidecode -t memory | grep Locator

Locator: DIMM_A1
Bank Locator: NODE 1
Locator: DIMM_A2
Bank Locator: NODE 1
Locator: DIMM_B1
Bank Locator: NODE 1
Locator: DIMM_B2
Bank Locator: NODE 1
...
Locator: DIMM_G1
Bank Locator: NODE 2
Locator: DIMM_G2
Bank Locator: NODE 2
Locator: DIMM_H1
Bank Locator: NODE 2
Locator: DIMM_H2
Bank Locator: NODE 2
```

上面的示例输出显示了8个通道，从`A`到`H`，每个通道都有2个DIMM。

您还可以使用`dmidecode`来确定内存频率：

```sh
dmidecode -t memory | grep Speed

Speed: 2133 MHz
Configured Clock Speed: 2134 MHz
Speed: Unknown
Configured Clock Speed: Unknown
Speed: 2133 MHz
Configured Clock Speed: 2134 MHz
Speed: Unknown
...
Speed: 2133 MHz
Configured Clock Speed: 2134 MHz
Speed: Unknown
Configured Clock Speed: Unknown
Speed: 2133 MHz
Configured Clock Speed: 2134 MHz
Speed: Unknown
Configured Clock Speed: Unknown
```

输出显示2133 MHz（DDR4）和未知（不存在）的速度。 这与先前的输出一致，表明每个通道都有一个记忆棒。

### 7.1.1. 网卡要求

使用[DPDK支持](http://dpdk.org/doc/nics)的高端NIC，如Intel XL710 40GbE。

确保每个NIC已经刷新最新版本的NVM /固件。

使用PCIe Gen3插槽，如Gen3 `x8`或Gen3 `x16`，因为PCIe Gen2插槽不能为2 x 10GbE及以上的带宽提供足够的带宽。 您可以使用`lspci`来检查PCI插槽的速度，方法如下：

```sh
lspci -s 03:00.1 -vv | grep LnkSta

LnkSta: Speed 8GT/s, Width x8, TrErr- Train- SlotClk+ DLActive- ...
LnkSta2: Current De-emphasis Level: -6dB, EqualizationComplete+ ...
```

将NIC插入PCI插槽时，始终检查标签，如CPU0或CPU1，以指示其连接的插槽。

插槽为NUMA情况时应该注意。 如果从不同的NIC使用2个或更多端口，最好确保这些NIC位于同一个CPU插槽上。 下面进一步显示了如何确定这一点的示例。

### 7.1.2. BIOS设置

以下是有关BIOS设置的一些建议。 不同的平台具有不同的BIOS命名，所以以下主要用于参考：

1. 开始之前，请考虑将所有BIOS设置重置为默认值。
2. 禁用所有省电选项，如电源性能调整， CPU P-State， CPU C3 Report 和 CPU C6 Report。
3. 选择**Performance**作为CPU Power和Performance 策略。
4. 禁用Turbo Boost以确保性能缩放随核心数量的增加而增加。
5. 将内存频率设置为最高可用值，不自动。
6. 测试NIC的物理功能时禁用所有虚拟化选项，如果要使用VFIO，请打开`VT-d`。

### 7.1.3. Linux启动命令行

以下是有关GRUB启动设置的一些建议：

1. 使用默认的grub文件作为起点。

2. 通过grub配置保留1G hugepages。 例如，保留8个1G的hugepages：

   ```sh
   default_hugepagesz=1G hugepagesz=1G hugepages=8
   ```

3. 隔离将用于DPDK的CPU内核。 例如：

   ```sh
   isolcpus=2,3,4,5,6,7,8
   ```

4. 如果要使用VFIO，请使用以下附加的grub参数：

   ```sh
   iommu=pt intel_iommu=on
   ```

## 7.2. Configurations before running DPDK

1. Build the DPDK target and reserve huge pages. See the earlier section on [Use of Hugepages in the Linux Environment](http://dpdk.org/doc/guides/linux_gsg/sys_reqs.html#linux-gsg-hugepages) for more details.

   The following shell commands may help with building and configuration:

   ```
   # Build DPDK target.
   cd dpdk_folder
   make install T=x86_64-native-linuxapp-gcc -j

   # Get the hugepage size.
   awk '/Hugepagesize/ {print $2}' /proc/meminfo

   # Get the total huge page numbers.
   awk '/HugePages_Total/ {print $2} ' /proc/meminfo

   # Unmount the hugepages.
   umount `awk '/hugetlbfs/ {print $2}' /proc/mounts`

   # Create the hugepage mount folder.
   mkdir -p /mnt/huge

   # Mount to the specific folder.
   mount -t hugetlbfs nodev /mnt/huge

   ```

2. Check the CPU layout using using the DPDK `cpu_layout` utility:

   ```
   cd dpdk_folder

   usertools/cpu_layout.py

   ```

   Or run `lscpu` to check the the cores on each socket.

3. Check your NIC id and related socket id:

   ```
   # List all the NICs with PCI address and device IDs.
   lspci -nn | grep Eth

   ```

   For example suppose your output was as follows:

   ```
   82:00.0 Ethernet [0200]: Intel XL710 for 40GbE QSFP+ [8086:1583]
   82:00.1 Ethernet [0200]: Intel XL710 for 40GbE QSFP+ [8086:1583]
   85:00.0 Ethernet [0200]: Intel XL710 for 40GbE QSFP+ [8086:1583]
   85:00.1 Ethernet [0200]: Intel XL710 for 40GbE QSFP+ [8086:1583]

   ```

   Check the PCI device related numa node id:

   ```
   cat /sys/bus/pci/devices/0000\:xx\:00.x/numa_node

   ```

   Usually `0x:00.x` is on socket 0 and `8x:00.x` is on socket 1. **Note**: To get the best performance, ensure that the core and NICs are in the same socket. In the example above `85:00.0` is on socket 1 and should be used by cores on socket 1 for the best performance.

4. Bind the test ports to DPDK compatible drivers, such as igb_uio. For example bind two ports to a DPDK compatible driver and check the status:

   ```
   # Bind ports 82:00.0 and 85:00.0 to dpdk driver
   ./dpdk_folder/usertools/dpdk-devbind.py -b igb_uio 82:00.0 85:00.0

   # Check the port driver status
   ./dpdk_folder/usertools/dpdk-devbind.py --status

   ```

   See `dpdk-devbind.py --help` for more details.

More details about DPDK setup and Linux kernel requirements see [Compiling the DPDK Target from Source](http://dpdk.org/doc/guides/linux_gsg/build_dpdk.html#linux-gsg-compiling-dpdk).

## 7.3. Example of getting best performance for an Intel NIC

The following is an example of running the DPDK `l3fwd` sample application to get high performance with an Intel server platform and Intel XL710 NICs. For specific 40G NIC configuration please refer to the i40e NIC guide.

The example scenario is to get best performance with two Intel XL710 40GbE ports. See [Fig. 7.1](http://dpdk.org/doc/guides/linux_gsg/nic_perf_intel_platform.html#figure-intel-perf-test-setup) for the performance test setup.

Fig. 7.1 Performance Test Setup

1. Add two Intel XL710 NICs to the platform, and use one port per card to get best performance. The reason for using two NICs is to overcome a PCIe Gen3’s limitation since it cannot provide 80G bandwidth for two 40G ports, but two different PCIe Gen3 x8 slot can. Refer to the sample NICs output above, then we can select `82:00.0` and `85:00.0` as test ports:

   ```
   82:00.0 Ethernet [0200]: Intel XL710 for 40GbE QSFP+ [8086:1583]
   85:00.0 Ethernet [0200]: Intel XL710 for 40GbE QSFP+ [8086:1583]

   ```

2. Connect the ports to the traffic generator. For high speed testing, it’s best to use a hardware traffic generator.

3. Check the PCI devices numa node (socket id) and get the cores number on the exact socket id. In this case, `82:00.0` and `85:00.0` are both in socket 1, and the cores on socket 1 in the referenced platform are 18-35 and 54-71. Note: Don’t use 2 logical cores on the same core (e.g core18 has 2 logical cores, core18 and core54), instead, use 2 logical cores from different cores (e.g core18 and core19).

4. Bind these two ports to igb_uio.

5. As to XL710 40G port, we need at least two queue pairs to achieve best performance, then two queues per port will be required, and each queue pair will need a dedicated CPU core for receiving/transmitting packets.

6. The DPDK sample application `l3fwd` will be used for performance testing, with using two ports for bi-directional forwarding. Compile the `l3fwd sample` with the default lpm mode.

7. The command line of running l3fwd would be something like the followings:

   ```
   ./l3fwd -l 18-21 -n 4 -w 82:00.0 -w 85:00.0 \
           -- -p 0x3 --config '(0,0,18),(0,1,19),(1,0,20),(1,1,21)'

   ```

   This means that the application uses core 18 for port 0, queue pair 0 forwarding, core 19 for port 0, queue pair 1 forwarding, core 20 for port 1, queue pair 0 forwarding, and core 21 for port 1, queue pair 1 forwarding.

8. Configure the traffic at a traffic generator.

   - Start creating a stream on packet generator.
   - Set the Ethernet II type to 0x0800.