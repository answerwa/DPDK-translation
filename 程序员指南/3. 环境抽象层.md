# 3. [环境抽象层](http://dpdk.org/doc/guides/prog_guide/env_abstraction_layer.html)

环境抽象层（EAL）负责获取对硬件和内存空间等低级资源的访问。 它提供了一个通用接口，可以隐藏应用程序和库的环境细节。 初始化例程有责任决定如何分配这些资源（即内存空间，PCI设备，计时器，控制台等）。

来自EAL的典型服务包括：

- DPDK 加载和启动：DPDK 及其应用程序作为单个应用程序链接，必须通过某种方式加载。
- 核心关联/分配过程：EAL 提供了将执行单元分配给特定内核以及创建执行实例的机制。
- 系统内存预留：EAL有助于预留不同的内存区域，例如设备交互的物理内存区域。
- PCI地址抽象：EAL提供了访问PCI地址空间的接口。
- 跟踪和调试功能：日志，dump_stack，panic 等。
- 实用程序功能：libc 中未提供的旋转锁和原子计数器。
- CPU 功能标识：在运行时确定是否支持特定功能，例如 Intel®AVX。确定当前CPU是否支持二进制编译的功能集。
- 中断处理：向特定中断源注册/注销回调的接口。
- 报警功能：设置/删除要在特定时间运行的回调的接口。

## 3.1. Linux用户执行环境中的EAL

在 Linux 用户空间环境中，DPDK 应用程序使用 pthread 库作为用户空间应用程序运行。有关设备和地址空间的 PCI 信息通过`/sys`内核接口和内核模块（如 uio_pci_generic 或 igb_uio）发现。请参阅Linux内核中的UIO：用户空间驱动程序文档。 这个内存在应用程序中已映射。

EAL使用 hugetlbfs 中的 mmap() 进行物理内存分配（使用巨页来提高性能）。该内存暴露于DPDK服务层，如[Mempool 库](https://github.com/answerwa/DPDK-translation/blob/master/%E7%A8%8B%E5%BA%8F%E5%91%98%E6%8C%87%E5%8D%97/5.%20%E5%86%85%E5%AD%98%E6%B1%A0%E5%BA%93.md)。

此时，DPDK 服务层将被初始化，然后通过 pthread setaffinity 调用，每个执行单元将分配给一个特定的逻辑内核作为用户级线程运行。

时间参考由 CPU 时间戳计数器（TSC）或 HPET 内核 API 通过 mmap() 调用提供。

### 3.1.1. 初始化和核心启动

初始化的一部分由glibc的启动函数完成。在初始化时也执行检查，以确保 CPU 支持配置文件中选择的微架构类型。然后，main() 函数被调用。 核心初始化和启动在 rte_eal_init() 中完成（参见API文档）。它包括对pthread库的调用（更具体地说是pthread_self()，pthread_create() 和 pthread_setaffinity_np()）。

![](http://dpdk.org/doc/guides/_images/linuxapp_launch.svg)

Fig. 3.1 Linux应用程序环境中的EAL初始化

> **注意**
>
> 作为主核心上的整体应用程序初始化的一部分，应对对象进行初始化，如内存区域，环，内存池，lpm 表和哈希表。这些对象的创建和初始化函数不是多线程安全的。但一旦初始化，对象本身可以安全地同时在多个线程中使用。

### 3.1.2. 支持多进程

Linuxapp EAL 允许多进程以及多线程（pthread）部署模型。 有关详细信息，请参见[支持多进程](https://github.com/answerwa/DPDK-translation/blob/master/程序员指南/20.%20多进程支持.md)章节。

### 3.1.3. 内存映射探索和内存预留

大型连续物理内存的分配是使用 hugetlbfs 内核文件系统完成的。EAL提供了一个API来在这个连续的内存中保留命名的内存区域。该内存区域的保留物理地址也由内存区域预留 API 返回给用户。

> **注意**
>
> 使用 rte_malloc 提供的 API 完成的内存预留也由 hugetlbfs 文件系统的页面支持。

### 3.1.4. 无 hugetbls 支持 Xen Dom0

现有的内存管理实现是基于 Linux 内核的 hugepage 机制。但是，Xen Dom0 不支持 hugepages，因此添加了一个新的 Linux 内核模块 rte_dom0_mm 来解决这个限制。

EAL使用 IOCTL 接口通知 Linux 内核模块 rte_dom0_mm 分配指定大小的内存，并从模块中获取所有内存段信息，EAL 使用 MMAP 接口映射已分配的内存。对于每个内存段，物理地址在其内部是连续的，但实际的硬件地址在 2 MB 内是连续的。

### 3.1.5. PCI 访问

EAL使用内核提供的 `/sys/bus/pci` 实用程序来扫描 PCI 总线上的内容。为了访问 PCI 内存，一个名为 uio_pci_generic 的内核模块提供 `/dev/uioX` 设备文件和 `/sys` 中的资源文件，可以从应用程序中获取对PCI地址空间的访问权限。DPDK 特定的 igb_uio 模块也可以用于此。这两款驱动都使用 uio 内核功能（用户驱动程序）。

### 3.1.6. Per-lcore 和共享变量

> **注意**
>
> lcore是指处理器的逻辑执行单元，有时称为硬件线程。

共享变量是默认行为。 Per-lcore变量使用线程本地存储（TLS）来实现，以提供 per-thread 本地存储。

### 3.1.7. 日志

日志 API 由 EAL 提供。 默认情况下，在 Linux 应用程序中，日志将发送到 syslog，也发送到控制台。 但是，日志功能可以被用户覆盖，以使用不同的日志记录机制。

#### 3.1.7.1. 跟踪和调试功能

在 glibc 中有一些调试函数可以将栈转存。rte_panic() 函数可以自发地引发 SIG_ABORT，它可以触发由gdb读取的core文件的生成。

### 3.1.8. CPU 功能识别

EAL可以在运行时查询CPU（使用 rte_cpu_get_feature() 函数）来确定哪些 CPU 功能可用。

### 3.1.9. 用户空间中断事件

- 主机线程中的用户空间中断和报警处理

EAL 创建一个主机线程来轮询 UIO 设备文件描述符以检测中断。回调可以通过特定中断事件的EAL函数进行注册或取消注册，并以异步方式在主机线程中调用。EAL 还允许以与 NIC 中断相同的方式使用定时回调。

> **注意**
>
> 在 DPDK PMD 中，由专用主机线程处理的唯一中断是链路状态更改（链接建立和链接关闭通知）以及突然的设备移除。

- RX 中断事件

每个 PMD 提供的接收和发送例程不限制自己在轮询线程模式下执行。为了在微小的吞吐量时缓解空闲轮询，暂停轮询并等待直到唤醒事件发生是很有用的。RX 中断是这种唤醒事件的首选，但可能不会是唯一的。

EAL 为事件驱动的线程模式提供事件 API。以 linuxapp 为例，实现依赖于 epoll。每个线程都可以监视一个 epoll 实例，其中添加了所有唤醒事件的文件描述符。事件文件描述符根据 UIO/VFIO 规范创建并映射到中断向量中。从 bsdapp 的角度来看，kqueue 是替代方法，但尚未实现。

EAL 初始化事件文件描述符和中断向量之间的映射，而每个设备初始化中断向量和队列之间的映射。这样，EAL 实际上并不知道特定向量的中断原因。eth_dev 驱动程序负责编写后一个映射。

> **注意**
>
> 每个队列 RX 中断事件只允许在支持多个 MSI-X 向量的 VFIO 中。在 UIO 中，RX 中断与其他中断原因共享相同的向量。在这种情况下，当 RX 中断和 LSC（链路状态改变）中断都被使能（intr_conf.lsc == 1 && intr_conf.rxq == 1）时，只有前者才能生效。

RX 中断由 ethdev API - `rte_eth_dev_rx_intr_ *` 控制/启用/禁用 。如果 PMD 尚未支持，则返回失败。`intr_conf.rxq` 标志用于打开每个设备的RX中断功能。

- 设备移除事件

该事件由在总线级别被移除的设备触发。其底层资源可能不可用（即未映射的 PCI 映射）。PMD 必须确保在这种情况下，应用程序仍然可以安全地使用其回调。

此事件可以以与订阅链接状态更改事件相同的方式订阅。执行上下文因此是相同的，即它是专用的中断主机线程。

考虑到这一点，应用程序很可能想要关闭已经发出设备删除事件的设备。在这种情况下，调用 `rte_eth_dev_close()` 可以触发它取消注册自己的设备删除事件回调。必须注意不要从中断处理程序上下文关闭设备。有必要重新安排这种关闭操作。

### 3.1.10. Blacklisting

The EAL PCI device blacklist functionality can be used to mark certain NIC ports as blacklisted, so they are ignored by the DPDK. The ports to be blacklisted are identified using the PCIe* description (Domain:Bus:Device.Function).

### 3.1.11. Misc Functions

Locks and atomic operations are per-architecture (i686 and x86_64).

## 3.2. Memory Segments and Memory Zones (memzone)

The mapping of physical memory is provided by this feature in the EAL. As physical memory can have gaps, the memory is described in a table of descriptors, and each descriptor (called rte_memseg ) describes a contiguous portion of memory.

On top of this, the memzone allocator’s role is to reserve contiguous portions of physical memory. These zones are identified by a unique name when the memory is reserved.

The rte_memzone descriptors are also located in the configuration structure. This structure is accessed using rte_eal_get_configuration(). The lookup (by name) of a memory zone returns a descriptor containing the physical address of the memory zone.

Memory zones can be reserved with specific start address alignment by supplying the align parameter (by default, they are aligned to cache line size). The alignment value should be a power of two and not less than the cache line size (64 bytes). Memory zones can also be reserved from either 2 MB or 1 GB hugepages, provided that both are available on the system.

## 3.3. Multiple pthread

DPDK usually pins one pthread per core to avoid the overhead of task switching. This allows for significant performance gains, but lacks flexibility and is not always efficient.

Power management helps to improve the CPU efficiency by limiting the CPU runtime frequency. However, alternately it is possible to utilize the idle cycles available to take advantage of the full capability of the CPU.

By taking advantage of cgroup, the CPU utilization quota can be simply assigned. This gives another way to improve the CPU efficiency, however, there is a prerequisite; DPDK must handle the context switching between multiple pthreads per core.

For further flexibility, it is useful to set pthread affinity not only to a CPU but to a CPU set.

### 3.3.1. EAL pthread and lcore Affinity

The term “lcore” refers to an EAL thread, which is really a Linux/FreeBSD pthread. “EAL pthreads” are created and managed by EAL and execute the tasks issued by *remote_launch*. In each EAL pthread, there is a TLS (Thread Local Storage) called *_lcore_id* for unique identification. As EAL pthreads usually bind 1:1 to the physical CPU, the *_lcore_id* is typically equal to the CPU ID.

When using multiple pthreads, however, the binding is no longer always 1:1 between an EAL pthread and a specified physical CPU. The EAL pthread may have affinity to a CPU set, and as such the *_lcore_id* will not be the same as the CPU ID. For this reason, there is an EAL long option ‘–lcores’ defined to assign the CPU affinity of lcores. For a specified lcore ID or ID group, the option allows setting the CPU set for that EAL pthread.

- The format pattern:

  –lcores=’<lcore_set>[@cpu_set][,<lcore_set>[@cpu_set],...]’

‘lcore_set’ and ‘cpu_set’ can be a single number, range or a group.

A number is a “digit([0-9]+)”; a range is “<number>-<number>”; a group is “(<number|range>[,<number|range>,...])”.

If a ‘@cpu_set’ value is not supplied, the value of ‘cpu_set’ will default to the value of ‘lcore_set’.

> ```
> For example, "--lcores='1,2@(5-7),(3-5)@(0,2),(0,6),7-8'" which means start 9 EAL thread;
>     lcore 0 runs on cpuset 0x41 (cpu 0,6);
>     lcore 1 runs on cpuset 0x2 (cpu 1);
>     lcore 2 runs on cpuset 0xe0 (cpu 5,6,7);
>     lcore 3,4,5 runs on cpuset 0x5 (cpu 0,2);
>     lcore 6 runs on cpuset 0x41 (cpu 0,6);
>     lcore 7 runs on cpuset 0x80 (cpu 7);
>     lcore 8 runs on cpuset 0x100 (cpu 8).
>
> ```

Using this option, for each given lcore ID, the associated CPUs can be assigned. It’s also compatible with the pattern of corelist(‘-l’) option.

### 3.3.2. non-EAL pthread support

It is possible to use the DPDK execution context with any user pthread (aka. Non-EAL pthreads). In a non-EAL pthread, the *_lcore_id* is always LCORE_ID_ANY which identifies that it is not an EAL thread with a valid, unique, *_lcore_id*. Some libraries will use an alternative unique ID (e.g. TID), some will not be impacted at all, and some will work but with limitations (e.g. timer and mempool libraries).

All these impacts are mentioned in [Known Issues](http://dpdk.org/doc/guides/prog_guide/env_abstraction_layer.html#known-issue-label) section.

### 3.3.3. Public Thread API

There are two public APIs `rte_thread_set_affinity()` and `rte_pthread_get_affinity()` introduced for threads. When they’re used in any pthread context, the Thread Local Storage(TLS) will be set/get.

Those TLS include *_cpuset* and *_socket_id*:

- *_cpuset* stores the CPUs bitmap to which the pthread is affinitized.
- *_socket_id* stores the NUMA node of the CPU set. If the CPUs in CPU set belong to different NUMA node, the *_socket_id* will be set to SOCKET_ID_ANY.

### 3.3.4. Known Issues

- rte_mempool

  The rte_mempool uses a per-lcore cache inside the mempool. For non-EAL pthreads, `rte_lcore_id()` will not return a valid number. So for now, when rte_mempool is used with non-EAL pthreads, the put/get operations will bypass the default mempool cache and there is a performance penalty because of this bypass. Only user-owned external caches can be used in a non-EAL context in conjunction with `rte_mempool_generic_put()` and `rte_mempool_generic_get()`that accept an explicit cache parameter.

- rte_ring

  rte_ring supports multi-producer enqueue and multi-consumer dequeue. However, it is non-preemptive, this has a knock on effect of making rte_mempool non-preemptable.

  Note

  The “non-preemptive” constraint means:

  - a pthread doing multi-producers enqueues on a given ring must not be preempted by another pthread doing a multi-producer enqueue on the same ring.
  - a pthread doing multi-consumers dequeues on a given ring must not be preempted by another pthread doing a multi-consumer dequeue on the same ring.

  Bypassing this constraint may cause the 2nd pthread to spin until the 1st one is scheduled again. Moreover, if the 1st pthread is preempted by a context that has an higher priority, it may even cause a dead lock.

  This does not mean it cannot be used, simply, there is a need to narrow down the situation when it is used by multi-pthread on the same core.

  1. It CAN be used for any single-producer or single-consumer situation.
  2. It MAY be used by multi-producer/consumer pthread whose scheduling policy are all SCHED_OTHER(cfs). User SHOULD be aware of the performance penalty before using it.
  3. It MUST not be used by multi-producer/consumer pthreads, whose scheduling policies are SCHED_FIFO or SCHED_RR.

- rte_timer

  Running `rte_timer_manager()` on a non-EAL pthread is not allowed. However, resetting/stopping the timer from a non-EAL pthread is allowed.

- rte_log

  In non-EAL pthreads, there is no per thread loglevel and logtype, global loglevels are used.

- misc

  The debug statistics of rte_ring, rte_mempool and rte_timer are not supported in a non-EAL pthread.

### 3.3.5. cgroup control

The following is a simple example of cgroup control usage, there are two pthreads(t0 and t1) doing packet I/O on the same core ($CPU). We expect only 50% of CPU spend on packet IO.

> ```
> mkdir /sys/fs/cgroup/cpu/pkt_io
> mkdir /sys/fs/cgroup/cpuset/pkt_io
>
> echo $cpu > /sys/fs/cgroup/cpuset/cpuset.cpus
>
> echo $t0 > /sys/fs/cgroup/cpu/pkt_io/tasks
> echo $t0 > /sys/fs/cgroup/cpuset/pkt_io/tasks
>
> echo $t1 > /sys/fs/cgroup/cpu/pkt_io/tasks
> echo $t1 > /sys/fs/cgroup/cpuset/pkt_io/tasks
>
> cd /sys/fs/cgroup/cpu/pkt_io
> echo 100000 > pkt_io/cpu.cfs_period_us
> echo  50000 > pkt_io/cpu.cfs_quota_us
>
> ```

## 3.4. Malloc

The EAL provides a malloc API to allocate any-sized memory.

The objective of this API is to provide malloc-like functions to allow allocation from hugepage memory and to facilitate application porting. The *DPDK API Reference* manual describes the available functions.

Typically, these kinds of allocations should not be done in data plane processing because they are slower than pool-based allocation and make use of locks within the allocation and free paths. However, they can be used in configuration code.

Refer to the rte_malloc() function description in the *DPDK API Reference* manual for more information.

### 3.4.1. Cookies

When CONFIG_RTE_MALLOC_DEBUG is enabled, the allocated memory contains overwrite protection fields to help identify buffer overflows.

### 3.4.2. Alignment and NUMA Constraints

The rte_malloc() takes an align argument that can be used to request a memory area that is aligned on a multiple of this value (which must be a power of two).

On systems with NUMA support, a call to the rte_malloc() function will return memory that has been allocated on the NUMA socket of the core which made the call. A set of APIs is also provided, to allow memory to be explicitly allocated on a NUMA socket directly, or by allocated on the NUMA socket where another core is located, in the case where the memory is to be used by a logical core other than on the one doing the memory allocation.

### 3.4.3. Use Cases

This API is meant to be used by an application that requires malloc-like functions at initialization time.

For allocating/freeing data at runtime, in the fast-path of an application, the memory pool library should be used instead.

### 3.4.4. Internal Implementation

#### 3.4.4.1. Data Structures

There are two data structure types used internally in the malloc library:

- struct malloc_heap - used to track free space on a per-socket basis
- struct malloc_elem - the basic element of allocation and free-space tracking inside the library.

##### 3.4.4.1.1. Structure: malloc_heap

The malloc_heap structure is used to manage free space on a per-socket basis. Internally, there is one heap structure per NUMA node, which allows us to allocate memory to a thread based on the NUMA node on which this thread runs. While this does not guarantee that the memory will be used on that NUMA node, it is no worse than a scheme where the memory is always allocated on a fixed or random node.

The key fields of the heap structure and their function are described below (see also diagram above):

- lock - the lock field is needed to synchronize access to the heap. Given that the free space in the heap is tracked using a linked list, we need a lock to prevent two threads manipulating the list at the same time.
- free_head - this points to the first element in the list of free nodes for this malloc heap.

Note

The malloc_heap structure does not keep track of in-use blocks of memory, since these are never touched except when they are to be freed again - at which point the pointer to the block is an input to the free() function.

Fig. 3.2 Example of a malloc heap and malloc elements within the malloc library

##### 3.4.4.1.2. Structure: malloc_elem

The malloc_elem structure is used as a generic header structure for various blocks of memory. It is used in three different ways - all shown in the diagram above:

1. As a header on a block of free or allocated memory - normal case
2. As a padding header inside a block of memory
3. As an end-of-memseg marker

The most important fields in the structure and how they are used are described below.

Note

If the usage of a particular field in one of the above three usages is not described, the field can be assumed to have an undefined value in that situation, for example, for padding headers only the “state” and “pad” fields have valid values.

- heap - this pointer is a reference back to the heap structure from which this block was allocated. It is used for normal memory blocks when they are being freed, to add the newly-freed block to the heap’s free-list.
- prev - this pointer points to the header element/block in the memseg immediately behind the current one. When freeing a block, this pointer is used to reference the previous block to check if that block is also free. If so, then the two free blocks are merged to form a single larger block.
- next_free - this pointer is used to chain the free-list of unallocated memory blocks together. It is only used in normal memory blocks; on `malloc()` to find a suitable free block to allocate and on `free()` to add the newly freed element to the free-list.
- state - This field can have one of three values: `FREE`, `BUSY` or `PAD`. The former two are to indicate the allocation state of a normal memory block and the latter is to indicate that the element structure is a dummy structure at the end of the start-of-block padding, i.e. where the start of the data within a block is not at the start of the block itself, due to alignment constraints. In that case, the pad header is used to locate the actual malloc element header for the block. For the end-of-memseg structure, this is always a `BUSY` value, which ensures that no element, on being freed, searches beyond the end of the memseg for other blocks to merge with into a larger free area.
- pad - this holds the length of the padding present at the start of the block. In the case of a normal block header, it is added to the address of the end of the header to give the address of the start of the data area, i.e. the value passed back to the application on a malloc. Within a dummy header inside the padding, this same value is stored, and is subtracted from the address of the dummy header to yield the address of the actual block header.
- size - the size of the data block, including the header itself. For end-of-memseg structures, this size is given as zero, though it is never actually checked. For normal blocks which are being freed, this size value is used in place of a “next” pointer to identify the location of the next block of memory that in the case of being `FREE`, the two free blocks can be merged into one.

#### 3.4.4.2. Memory Allocation

On EAL initialization, all memsegs are setup as part of the malloc heap. This setup involves placing a dummy structure at the end with `BUSY` state, which may contain a sentinel value if `CONFIG_RTE_MALLOC_DEBUG` is enabled, and a proper [element header](http://dpdk.org/doc/guides/prog_guide/env_abstraction_layer.html#malloc-elem) with `FREE` at the start for each memseg. The `FREE` element is then added to the `free_list` for the malloc heap.

When an application makes a call to a malloc-like function, the malloc function will first index the `lcore_config` structure for the calling thread, and determine the NUMA node of that thread. The NUMA node is used to index the array of `malloc_heap` structures which is passed as a parameter to the `heap_alloc()` function, along with the requested size, type, alignment and boundary parameters.

The `heap_alloc()` function will scan the free_list of the heap, and attempt to find a free block suitable for storing data of the requested size, with the requested alignment and boundary constraints.

When a suitable free element has been identified, the pointer to be returned to the user is calculated. The cache-line of memory immediately preceding this pointer is filled with a struct malloc_elem header. Because of alignment and boundary constraints, there could be free space at the start and/or end of the element, resulting in the following behavior:

1. Check for trailing space. If the trailing space is big enough, i.e. > 128 bytes, then the free element is split. If it is not, then we just ignore it (wasted space).
2. Check for space at the start of the element. If the space at the start is small, i.e. <=128 bytes, then a pad header is used, and the remaining space is wasted. If, however, the remaining space is greater, then the free element is split.

The advantage of allocating the memory from the end of the existing element is that no adjustment of the free list needs to take place - the existing element on the free list just has its size pointer adjusted, and the following element has its “prev” pointer redirected to the newly created element.

#### 3.4.4.3. Freeing Memory

To free an area of memory, the pointer to the start of the data area is passed to the free function. The size of the `malloc_elem` structure is subtracted from this pointer to get the element header for the block. If this header is of type `PAD` then the pad length is further subtracted from the pointer to get the proper element header for the entire block.

From this element header, we get pointers to the heap from which the block was allocated and to where it must be freed, as well as the pointer to the previous element, and via the size field, we can calculate the pointer to the next element. These next and previous elements are then checked to see if they are also `FREE`, and if so, they are merged with the current element. This means that we can never have two `FREE` memory blocks adjacent to one another, as they are always merged into a single block.